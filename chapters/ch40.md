> # 40. Generalizing from the training set to the dev set

--> _replace THIS LINE by your translation for the above line_


> Suppose you are applying ML in a setting where the training and the dev/test distributions are different. Say, the training set contains Internet images + Mobile images, and the dev/test sets contain only Mobile images. However, the algorithm is not working well: It has a much higher dev/test set error than you would like. Here are some possibilities of what might be wrong:

--> _replace THIS LINE by your translation for the above line_


> 1. It does not do well on the training set. This is the problem of high (avoidable) bias on the training set distribution.

--> _replace THIS LINE by your translation for the above line_


> 2. It does well on the training set, but does not generalize well to previously unseen data *drawn from the same distribution as the training set*.​ This is high variance.

--> _replace THIS LINE by your translation for the above line_


> 3. It generalizes well to new data drawn from the same distribution as the training set, but not to data drawn from the dev/test set distribution. We call this problem ​**data mismatch​**, since it is because the training set data is a poor match for the dev/test set data.

--> _replace THIS LINE by your translation for the above line_


> For example, suppose that humans achieve near perfect performance on the cat recognition task. Your algorithm achieves this:

--> _replace THIS LINE by your translation for the above line_


> * 1% error on the training set

--> _replace THIS LINE by your translation for the above line_


> * 1.5% error on data drawn from the same distribution as the training set that the algorithm has not seen

--> _replace THIS LINE by your translation for the above line_


> * 10% error on the dev set

--> _replace THIS LINE by your translation for the above line_


> In this case, you clearly have a data mismatch problem. To address this, you might try to make the training data more similar to the dev/test data. We discuss some techniques for this later.

--> _replace THIS LINE by your translation for the above line_


> In order to diagnose to what extent an algorithm suffers from each of the problems 1-3 above, it will be useful to have another dataset. Specifically, rather than giving the algorithm all the available training data, you can split it into two subsets: The actual training set which the algorithm will train on, and a separate set, which we will call the "Training dev" set, that we will not train on.

--> _replace THIS LINE by your translation for the above line_


> You now have four subsets of data:

--> _replace THIS LINE by your translation for the above line_


> * Training set. This is the data that the algorithm will learn from (e.g., Internet images + Mobile images). This does not have to be drawn from the same distribution as what we really care about (the dev/test set distribution).

--> _replace THIS LINE by your translation for the above line_


> * Training dev set: This data is drawn from the same distribution as the training set (e.g., Internet images + Mobile images). This is usually smaller than the training set; it only needs to be large enough to evaluate and track the progress of our learning algorithm.

--> _replace THIS LINE by your translation for the above line_


> * Dev set: This is drawn from the same distribution as the test set, and it reflects the distribution of data that we ultimately care about doing well on. (E.g., mobile images.)

--> _replace THIS LINE by your translation for the above line_


> * Test set: This is drawn from the same distribution as the dev set. (E.g., mobile images.)

--> _replace THIS LINE by your translation for the above line_


> Armed with these four separate datasets, you can now evaluate:

--> _replace THIS LINE by your translation for the above line_


> * Training error, by evaluating on the training set.

--> _replace THIS LINE by your translation for the above line_


> * The algorithm’s ability to generalize to new data drawn from the training set distribution, by evaluating on the training dev set.

--> _replace THIS LINE by your translation for the above line_


> * The algorithm’s performance on the task you care about, by evaluating on the dev and/or test sets.

--> _replace THIS LINE by your translation for the above line_


> Most of the guidelines in Chapters 5-7 for picking the size of the dev set also apply to the training dev set.

--> _replace THIS LINE by your translation for the above line_

