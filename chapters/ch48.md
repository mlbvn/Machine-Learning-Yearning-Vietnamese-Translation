# 48. More end-to-end learning examples
->

Suppose you want to build a speech recognition system. You might build a system with three components:
->

![img](../imgs/C48_01.png)

The components work as follows:
->

1. Compute features: Extract hand-designed features, such as MFCC (​Mel-frequency cepstrum coefficients) features, ​which try to capture the content of an utterance while disregarding less relevant properties, such as the speaker’s pitch.
->

2. Phoneme recognizer: Some linguists believe that there are basic units of sound called “phonemes.” For example, the initial “k” sound in “keep” is the same phoneme as the “c” sound in “cake.” This system tries to recognize the phonemes in the audio clip.
->

3. Final recognizer: Take the sequence of recognized phonemes, and try to string them together into an output transcript.
->

In contrast, an end-to-end system might input an audio clip, and try to directly output the transcript:
->

![img](../imgs/C48_02.png)

So far, we have only described machine learning “pipelines” that are completely linear: the output is sequentially passed from one staged to the next. Pipelines can be more complex. For example, here is a simple architecture for an autonomous car:
->

![img](../imgs/C48_03.png)

It has three components: One detects other cars using the camera images; one detects pedestrians; then a final component plans a path for our own car that avoids the cars and pedestrians.
->

Not every component in a pipeline has to be learned. For example, the literature on “robot motion planning” has numerous algorithms for the final path planning step for the car. Many of these algorithms do not involve learning.
->

In contrast, and end-to-end approach might try to take in the sensor inputs and directly output the steering direction:
->

![img](../imgs/C48_04.png)

Even though end-to-end learning has seen many successes, it is not always the best approach. For example, end-to-end speech recognition works well. But I’m skeptical about end-to-end learning for autonomous driving. The next few chapters explain why.
->