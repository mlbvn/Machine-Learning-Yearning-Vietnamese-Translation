> # 48. More end-to-end learning examples

--> _replace THIS LINE by your translation for the above line_

> Suppose you want to build a speech recognition system. You might build a system with three components:

--> _replace THIS LINE by your translation for the above line_

![img](../imgs/C48_01.png)

> The components work as follows:

--> _replace THIS LINE by your translation for the above line_

> 1. Compute features: Extract hand-designed features, such as MFCC (​Mel-frequency cepstrum coefficients) features, ​which try to capture the content of an utterance while disregarding less relevant properties, such as the speaker’s pitch.

--> _replace THIS LINE by your translation for the above line_

> 2. Phoneme recognizer: Some linguists believe that there are basic units of sound called "phonemes." For example, the initial "k" sound in "keep" is the same phoneme as the "c" sound in "cake." This system tries to recognize the phonemes in the audio clip.

--> _replace THIS LINE by your translation for the above line_

> 3. Final recognizer: Take the sequence of recognized phonemes, and try to string them together into an output transcript.

--> _replace THIS LINE by your translation for the above line_

> In contrast, an end-to-end system might input an audio clip, and try to directly output the transcript:

--> _replace THIS LINE by your translation for the above line_

![img](../imgs/C48_02.png)

> So far, we have only described machine learning "pipelines" that are completely linear: the output is sequentially passed from one staged to the next. Pipelines can be more complex. For example, here is a simple architecture for an autonomous car:

--> _replace THIS LINE by your translation for the above line_

![img](../imgs/C48_03.png)

> It has three components: One detects other cars using the camera images; one detects pedestrians; then a final component plans a path for our own car that avoids the cars and pedestrians.

--> _replace THIS LINE by your translation for the above line_

> Not every component in a pipeline has to be learned. For example, the literature on "robot motion planning" has numerous algorithms for the final path planning step for the car. Many of these algorithms do not involve learning.

--> _replace THIS LINE by your translation for the above line_

> In contrast, and end-to-end approach might try to take in the sensor inputs and directly output the steering direction:

--> _replace THIS LINE by your translation for the above line_

![img](../imgs/C48_04.png)

> Even though end-to-end learning has seen many successes, it is not always the best approach. For example, end-to-end speech recognition works well. But I’m skeptical about end-to-end learning for autonomous driving. The next few chapters explain why.

--> _replace THIS LINE by your translation for the above line_
