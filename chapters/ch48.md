> # 48. More end-to-end learning examples

-> _start your translation RIGHT AFTER this line_
> Suppose you want to build a speech recognition system. You might build a system with three components:

-> _start your translation RIGHT AFTER this line_

> The components work as follows:

-> _start your translation RIGHT AFTER this line_
> 1. Compute features: Extract hand-designed features, such as MFCC (​Mel-frequency cepstrum coefficients) features, ​which try to capture the content of an utterance while disregarding less relevant properties, such as the speaker’s pitch.

-> _start your translation RIGHT AFTER this line_
> 2. Phoneme recognizer: Some linguists believe that there are basic units of sound called “phonemes.” For example, the initial “k” sound in “keep” is the same phoneme as the “c” sound in “cake.” This system tries to recognize the phonemes in the audio clip.

-> _start your translation RIGHT AFTER this line_
> 3. Final recognizer: Take the sequence of recognized phonemes, and try to string them together into an output transcript.

-> _start your translation RIGHT AFTER this line_
> In contrast, an end-to-end system might input an audio clip, and try to directly output the transcript:

-> _start your translation RIGHT AFTER this line_

> So far, we have only described machine learning “pipelines” that are completely linear: the output is sequentially passed from one staged to the next. Pipelines can be more complex. For example, here is a simple architecture for an autonomous car:

-> _start your translation RIGHT AFTER this line_

> It has three components: One detects other cars using the camera images; one detects pedestrians; then a final component plans a path for our own car that avoids the cars and pedestrians.

-> _start your translation RIGHT AFTER this line_
> Not every component in a pipeline has to be learned. For example, the literature on “robot motion planning” has numerous algorithms for the final path planning step for the car. Many of these algorithms do not involve learning.

-> _start your translation RIGHT AFTER this line_
> In contrast, and end-to-end approach might try to take in the sensor inputs and directly output the steering direction:

-> _start your translation RIGHT AFTER this line_

> Even though end-to-end learning has seen many successes, it is not always the best approach. For example, end-to-end speech recognition works well. But I’m skeptical about end-to-end learning for autonomous driving. The next few chapters explain why.

-> _start your translation RIGHT AFTER this line_