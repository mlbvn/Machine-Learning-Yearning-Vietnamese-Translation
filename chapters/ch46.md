> # 46. Reinforcement learning example

--> _replace THIS LINE by your translation for the above line_

![img](../imgs/C46_01.png)

> Suppose you are using machine learning to teach a helicopter to fly complex maneuvers. Here is a time-lapse photo of a computer-controller helicopter executing a landing with the engine turned off.

--> _replace THIS LINE by your translation for the above line_

> This is called an "autorotation" maneuver. It allows helicopters to land even if their engine unexpectedly fails. Human pilots practice this maneuver as part of their training. Your goal is to use a learning algorithm to fly the helicopter through a trajectory ​*T* t​hat ends in a safe landing.

--> _replace THIS LINE by your translation for the above line_

> To apply reinforcement learning, you have to develop a "Reward function" ​*R​*(.) that gives a score measuring how good each possible trajectory ​*T​* is. For example, if ​*T* ​results in the helicopter crashing, then perhaps the reward is ​*R(T)​* = -1,000—a huge negative reward. A trajectory ​*T​* resulting in a safe landing might result in a positive ​*R(T)* w​ ith the exact value depending on how smooth the landing was. The reward function ​*R*(​.) is typically chosen by hand to quantify how desirable different trajectories ​*T​* are. It has to trade off how bumpy the landing was, whether the helicopter landed in exactly the desired spot, how rough the ride down was for passengers, and so on. It is not easy to design good reward functions.

--> _replace THIS LINE by your translation for the above line_

> Given a reward function ​*R(T)*, ​the job of the reinforcement learning algorithm is to control the helicopter so that it achieves max​<sub>*T*</sub>​*R(T)*. ​However, reinforcement learning algorithms make many approximations and may not succeed in achieving this maximization.

--> _replace THIS LINE by your translation for the above line_

> Suppose you have picked some reward ​*R(.)​* and have run your learning algorithm. However,
> its performance appears far worse than your human pilot—the landings are bumpier and
> seem less safe than what a human pilot achieves. How can you tell if the fault is with the
> reinforcement learning algorithm—which is trying to carry out a trajectory that achieves
> max​<sub>*T*</sub>​*R(T)*—​ or if the fault is with the reward function—which is trying to measure as well as specify the ideal tradeoff between ride bumpiness and accuracy of landing spot?

--> _replace THIS LINE by your translation for the above line_

> To apply the Optimization Verification test, let *T*<sub>human</sub>​ be the trajectory achieved by the human pilot, and let *T*<sub>out</sub>​ be the trajectory achieved by the algorithm. According to our description above, *T*<sub>human</sub>​ is a superior trajectory to *T*<sub>out</sub>​. Thus, the key test is the following:
> Does it hold true that *R*(*T*<sub>human</sub>) > *R*(*T*<sub>out</sub>​)?

--> _replace THIS LINE by your translation for the above line_

> Case 1: If this inequality holds, then the reward function ​*R(​.)* is correctly rating *T*<sub>human</sub>​ as superior to *T*<sub>out</sub>​. But our reinforcement learning algorithm is finding the inferior *T*<sub>out</sub>​. This suggests that working on improving our reinforcement learning algorithm is worthwhile.

--> _replace THIS LINE by your translation for the above line_

> Case 2: The inequality does not hold: *R*(*T*<sub>human</sub>) ≤ *R*(*T*<sub>out</sub>​). This mean *R​(.)* assigns a worse score to *T*<sub>human</sub>​ even though it is the superior trajectory. You should work on improving ​*R​(.)* to better capture the tradeoffs that correspond to a good landing.

--> _replace THIS LINE by your translation for the above line_

> Many machine learning applications have this "pattern" of optimizing an approximate
> scoring function Score​<sub>x</sub>​(.) using an approximate search algorithm. Sometimes, there is no specified input ​*x*,​ so this reduces to just Score(.). In our example above, the scoring function was the reward function Score(​*T*)​ = R(​*T*)​ , and the optimization algorithm was the reinforcement learning algorithm trying to execute a good trajectory ​*T*.​

--> _replace THIS LINE by your translation for the above line_

> One difference between this and earlier examples is that, rather than comparing to an "optimal" output, you were instead comparing to human-level performance *T*<sub>human</sub>​. We assumed *T*<sub>human</sub>​ is pretty good, even if not optimal. In general, so long as you have some y* (in this example, *T*<sub>human</sub>​) that is a superior output to the performance of your current learning algorithm—even if it is not the "optimal" output—then the Optimization Verification test can indicate whether it is more promising to improve the optimization algorithm or the scoring function.

--> _replace THIS LINE by your translation for the above line_
