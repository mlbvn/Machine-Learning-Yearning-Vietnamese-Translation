# Establish a single-number evaluation metric for your team to optimize

->

# Thiết lập một phép đo để đánh giá bằng một con số để nhóm của bạn tối ưu hoá 

Classification accuracy is an example of a ​**single-number evaluation metric**​: You run your classifier on the dev set (or test set), and get back a single number about what fraction of examples it classified correctly. According to this metric, if classifier A obtains 97% accuracy, and classifier B obtains 90% accuracy, then we judge classifier A to be superior.

-> 

Độ chính xác trong phân loại là một ví dụ của **một phép đo đánh giá bằng một số**: bạn chạy thuật toán phân loại của bạn trên một tập phát triển (hoặc tập kiểm tra), rồi nhận được một số thể hiện số phần trăm mẫu thử được phân loại chính xác. Dựa vào phép đo này, nếu thuật toán phân loại A thu được 97% độ chính xác, và thuật toán phân loại B thu được 90% độ chính xác, thì chúng ta đánh giá cao thuật toán A hơn.

In contrast, Precision and Recall[3] is not a single-number evaluation metric: It gives two numbers for assessing your classifier. Having multiple-number evaluation metrics makes it harder to compare algorithms. Suppose your algorithms perform as follows:

->

Ngược lại, Precision và Recall không phải là một phép đo bằng một con số: có hai số được sử dụng để đánh giá thuật toán phân loại của bạn. Việc so sánh các thuật toán với nhau sẽ trở nên khó hơn với phép đo có nhiều chiều. Giả sử thuật toán của bạn thu về kết quả như sau:

| Classifier  | Percision | Recall |
| ----- | -------: | -------: |
| A  | 95%  | 90% |
| B  | 98%  | 85% |

Here, neither classifier is obviously superior, so it doesn’t immediately guide you toward picking one.

->

| Thuật Toán Phân Loại  | Percision | Recall |
| ----- | -------: | -------: |
| A  | 95%  | 90% |
| B  | 98%  | 85% |

Với kết quả này, không thuật toán nào tốt hơn một cách rõ ràng, vì vậy kết quả này không thể giúp bạn ngay lập tức chọn ra một thuật toán tốt hơn.

During development, your team will try a lot of ideas about algorithm architecture, model parameters, choice of features, etc. Having a **​single-number evaluation metric​**​ such as accuracy allows you to sort all your models according to their performance on this metric, and quickly decide what is working best.

-> 

Trong quá trình phát triển, nhóm của bạn sẽ thử rất nhiều ý tưởng liên quan đến cấu trúc thuật toán, tham số mô hình, lựa chọn features, v.v.. Có một **phép đo đánh giá bằng một số** như độ chính xác sẽ giúp cho bạn sắp xếp mô mình của mình dựa theo hiệu suất của chính qua phép đo này, và từ đó nhanh chóng quyết định thuật toán nào hoạt động tốt nhất.

If you really care about both Precision and Recall, I recommend using one of the standard ways to combine them into a single number. For example, one could take the average of precision and recall, to end up with a single number. Alternatively, you can compute the “F1 score,” which is a modified way of computing their average, and works better than simply taking the mean.[4]

->

Nếu bạn thực sự quan tâm đến cả Precision lẫn Recall. Tôi gợi ý sử dụng một trong những cách tiêu chuẩn để kết hợp chúng thành một số duy nhất. Ví dụ, một người có thể lấy giá trị trung bình của precision và recall, rồi thu về một số duy nhất. Hoặc thay vào đó, bạn có thể tính "điểm F1", một cách tính toán sửa đổi của giá trị trung bình, và thường hoạt động tốt hơn là chỉ đơn giản tính trung bình.

| Classifier  | Percision | Recall | F1 score |
| ----- | -------: | -------: | -----: |
| A  | 95%  | 90% | 92.4% |
| B  | 98%  | 85% | 91.0% |

Having a single-number evaluation metric speeds up your ability to make a decision when you are selecting among a large number of classifiers. It gives a clear preference ranking among all of them, and therefore a clear direction for progress.

->

| Thuật Toán Phân Loại | Percision | Recall | Điểm F1 |
| ----- | -------: | -------: | -----: |
| A  | 95%  | 90% | 92.4% |
| B  | 98%  | 85% | 91.0% |

Việc có một phép đo đánh giá bằng một số sẽ giúp tăng tốc khả năng đưa ra quyết định của bạn khi bạn đang lựa chọn trong một số lượng lớn thuật toán phân loại. Nó đưa ra ưu tiên rõ ràng trong việc phân hạng những thuật toán đó, và từ đó đưa ra những đường hướng rõ ràng trong phát triển. 

As a final example, suppose you are separately tracking the accuracy of your cat classifier in four key markets: (i) US, (ii) China, (iii) India, and (iv) Other. This gives four metrics. By taking an average or weighted average of these four numbers, you end up with a single number metric. Taking an average or weighted average is one of the most common ways to combine multiple metrics into one.

->

Một ví dụ cuối cùng, giả sử bạn đang theo dõi riêng biệt về độ chính xác của thuật toán phân loại mèo ở trong bốn thị trường trọng điểm: (i) Mĩ, (ii) Trung Quốc, (iii) Ấn Độ, và (iv) những nước khác. Bạn sẽ thu về bốn phép đo. Bằng cách lấy giá trị trung bình hoặc giá trị trung bình có trọng số của bốn phép đo này, bạn sẽ thu được một phép đo đánh giá bằng một số duy nhất. Việc lấy giá trị trung bình hoặc giá trị trung bình có trọng số là một trong những cách phổ biển nhất để kết hợp nhiều phép đo lại thành một.

**FOOTNOTE:**

[3] The Precision of a cat classifier is the fraction of images in the dev (or test) set it labeled as cats that really are cats. Its Recall is the percentage of all cat images in the dev (or test) set that it correctly labeled as a cat. There is often a tradeoff between having high precision and high recall.

->

[3] Precision của một thuật toán phân loại mèo là số phần trăm ảnh ở trong tập phát triển (hoặc tập kiểm tra) được thuật toán phân loại là mèo, và thực sự là mèo. Recall của thuật toán đó là số phần trăm của tất cả ảnh mèo ở trong tập phát triển (hoặc tập kiểm tra) được thuật toán phân loại chính xác là mèo. Thường có một sự đánh đổi giữa việc có điểm precision cao và điểm recall cao.

[4] If you want to learn more about the F1 score, see ​https://en.wikipedia.org/wiki/F1_score​. It is the “harmonic mean” between Precision and Recall, and is calculated as 2/((1/Precision)+(1/Recall)).

->

[4] Nếu bạn muốn đọc thêm về điểm F1, truy cập ​https://en.wikipedia.org/wiki/F1_score​. Điểm F1 là trung bình điều hoà của Precision và Recall, được tính bằng 2/((1/Precision) + (1/Recall))