# Establish a single-number evaluation metric for your team to optimize

->

# Thiết lập một phép đo đơn trị để nhóm của bạn tối ưu

Classification accuracy is an example of a ​**single-number evaluation metric**​: You run your classifier on the dev set (or test set), and get back a single number about what fraction of examples it classified correctly. According to this metric, if classifier A obtains 97% accuracy, and classifier B obtains 90% accuracy, then we judge classifier A to be superior.

-> 

Độ chính xác trong phân loại là một ví dụ của một **phép đo đơn trị**. Khi chạy bộ phân loại trên một tập phát triển (hoặc tập kiểm tra), và nhận về một số thể hiện số phần trăm mẫu được phân loại chính xác trên tổng số mẫu, con số này được định nghĩa là độ chính xác. Nếu thuật toán phân loại A thu được 97% độ chính xác, và thuật toán phân loại B thu được 90% độ chính xác, thuật toán A sẽ được đánh giá cao hơn dựa theo thông số này.

In contrast, Precision and Recall[3] is not a single-number evaluation metric: It gives two numbers for assessing your classifier. Having multiple-number evaluation metrics makes it harder to compare algorithms. Suppose your algorithms perform as follows:

->

Ngược lại, Precision và Recall không phải là một phép đo đơn trị: có hai chỉ số được sử dụng để đánh giá bộ phân loại. Việc so sánh các thuật toán với nhau sẽ trở nên khó hơn với những phép đo đa trị. Giả sử thuật toán trả về kết quả như sau:

| Classifier  | Percision | Recall |
| ----- | -------: | -------: |
| A  | 95%  | 90% |
| B  | 98%  | 85% |

Here, neither classifier is obviously superior, so it doesn’t immediately guide you toward picking one.

->

| Bộ Phân Loại  | Percision | Recall |
| ----- | -------: | -------: |
| A  | 95%  | 90% |
| B  | 98%  | 85% |

Ở đây, không thuật toán nào tốt hơn một cách rõ ràng, vì vậy dựa vào kết quả này ta không thể ngay lập tức chọn ra một bộ phân loại tốt hơn.

During development, your team will try a lot of ideas about algorithm architecture, model parameters, choice of features, etc. Having a **​single-number evaluation metric​**​ such as accuracy allows you to sort all your models according to their performance on this metric, and quickly decide what is working best.

-> 

Trong quá trình phát triển, nhóm của bạn sẽ thử rất nhiều ý tưởng liên quan đến cấu trúc thuật toán, tham số mô hình, lựa chọn các đặc trưng, v.v.. Việc có một **phép đo đơn trị** như độ chính xác sẽ giúp sắp xếp các mô mình đó dựa theo những kết quả trả về qua phép đo đó, rồi từ đó nhanh chóng quyết định mô hình nào hoạt động tốt nhất.

If you really care about both Precision and Recall, I recommend using one of the standard ways to combine them into a single number. For example, one could take the average of precision and recall, to end up with a single number. Alternatively, you can compute the “F1 score,” which is a modified way of computing their average, and works better than simply taking the mean.[4]

->

Nếu bạn thực sự quan tâm đến cả Precision lẫn Recall. Tôi gợi ý sử dụng một trong những cách tiêu chuẩn để kết hợp các chỉ số đó thành một số duy nhất. Ví dụ, một người có thể lấy giá trị trung bình của Precision và Recall rồi thu về một phép đo đơn trị. Hoặc thay vào đó, bạn có thể tính "điểm F1", một cách tính toán sửa đổi của trung bình cộng, và thường hoạt động tốt hơn việc chỉ lấy giá trị trung bình nêu trên.

| Classifier  | Percision | Recall | F1 score |
| ----- | -------: | -------: | -----: |
| A  | 95%  | 90% | 92.4% |
| B  | 98%  | 85% | 91.0% |

Having a single-number evaluation metric speeds up your ability to make a decision when you are selecting among a large number of classifiers. It gives a clear preference ranking among all of them, and therefore a clear direction for progress.

->

| Thuật Toán Phân Loại | Percision | Recall | Điểm F1 |
| ----- | -------: | -------: | -----: |
| A  | 95%  | 90% | 92.4% |
| B  | 98%  | 85% | 91.0% |

Việc có một phép đo đơn trị sẽ giúp tăng tốc khả năng đưa ra quyết định của bạn khi bạn phải lựa chọn trong một số lượng lớn bộ phân loại. Phép đo đơn trị đưa ra ưu tiên rõ ràng trong việc phân hạng những thuật toán đó, rồi tạo ra những đường hướng rõ ràng để phát triển. 

As a final example, suppose you are separately tracking the accuracy of your cat classifier in four key markets: (i) US, (ii) China, (iii) India, and (iv) Other. This gives four metrics. By taking an average or weighted average of these four numbers, you end up with a single number metric. Taking an average or weighted average is one of the most common ways to combine multiple metrics into one.

->

Một ví dụ cuối cùng, giả sử bạn đang theo dõi riêng biệt về độ chính xác của thuật toán phân loại mèo ở trong bốn thị trường trọng điểm: (i) Mĩ, (ii) Trung Quốc, (iii) Ấn Độ, và (iv) những nước khác. Bạn sẽ thu về bốn phép đo. Bằng cách lấy giá trị trung bình hoặc giá trị trung bình trọng số của bốn phép đo này, bạn sẽ thu được một phép đo đơn trị. Tính toán giá trị trung bình hoặc giá trị trung bình trọng số là một trong những cách phổ biển nhất để kết hợp nhiều phép đo thành một.

**FOOTNOTE:**

[3] The Precision of a cat classifier is the fraction of images in the dev (or test) set it labeled as cats that really are cats. Its Recall is the percentage of all cat images in the dev (or test) set that it correctly labeled as a cat. There is often a tradeoff between having high precision and high recall.

->

[3] Precision của một thuật toán phân loại mèo là số phần trăm ảnh ở trong tập phát triển (hoặc tập kiểm tra) được thuật toán phân loại là mèo, và thực sự là mèo. Recall của thuật toán đó là số phần trăm của tất cả ảnh mèo ở trong tập phát triển (hoặc tập kiểm tra) được thuật toán phân loại chính xác là mèo. Thường có một sự đánh đổi giữa việc có chỉ số precision cao và chỉ số recall cao.

[4] If you want to learn more about the F1 score, see ​https://en.wikipedia.org/wiki/F1_score​. It is the “harmonic mean” between Precision and Recall, and is calculated as 2/((1/Precision)+(1/Recall)).

->

[4] Nếu bạn muốn đọc thêm về điểm F1, xem ​https://en.wikipedia.org/wiki/F1_score​. Điểm F1 là trung bình điều hoà của Precision và Recall, được tính bằng 2/((1/Precision) + (1/Recall))