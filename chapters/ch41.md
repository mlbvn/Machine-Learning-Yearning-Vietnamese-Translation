# 41. Identifying Bias, Variance, and Data Mismatch Errors
->


Suppose humans achieve almost perfect performance (≈0% error) on the cat detection task, and thus the optimal error rate is about 0%. Suppose you have:
->

* 1% error on the training set.
->


* 5% error on training dev set.
->


* 5% error on the dev set.
->

What does this tell you? Here, you know that you have high variance. The variance reduction techniques described earlier should allow you to make progress.
->


Now, suppose your algorithm achieves:
->


* 10% error on the training set.
->


* 11% error on training dev set.
->


* 12% error on the dev set.
->


This tells you that you have high avoidable bias on the training set. I.e., the algorithm is doing poorly on the training set. Bias reduction techniques should help.
->


In the two examples above, the algorithm suffered from only high avoidable bias or high variance. It is possible for an algorithm to suffer from any subset of high avoidable bias, high variance, and data mismatch. For example:
->


* 10% error on the training set.
->


* 11% error on training dev set.
->


* 20% error on the dev set.
->


This algorithm suffers from high avoidable bias and from data mismatch. It does not, however, suffer from high variance on the training set distribution.
->


It might be easier to understand how the different types of errors relate to each other by drawing them as entries in a table:
->


![img](../imgs/C41_01.png)

Continuing with the example of th​e cat image detector, you can see that there are two different distributions of data on the x-axis. On the y-axis, we ha​ve three types of error: human level error, error on examples the algorithm has trained on, and error on examples the algorithm has not trained on. We can fill in the boxes with the different types of errors we identified in the previous chapter.
->


If you wish, you can also fill in the remaining two boxes in this table: You can fill in the upper-right box (Human level performance on Mobile Images) by asking some humans to label your mobile cat images data and measure their error. You can fill in the next box by taking the mobile cat images (Distribution B) and putting a small fraction of into the training set so that the neural network learns on it too. Then you measure the learned model’s error on that subset of data. Filling in these two additional entries may sometimes give additional insight about what the algorithm is doing on the two different distributions (Distribution A and B) of data.
->


By understanding which types of error the algorithm suffers from the most, you will be better positioned to decide whether to focus on reducing bias, reducing variance, or reducing data mismatch.
->

